# Introduction
A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, I was tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms the data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to.

# Project Datasets
You'll be working with two datasets that reside in S3. Here are the S3 links for each:
- Song data: `s3://udacity-dend/song_data`
- Log data: `s3://udacity-dend/log_data`

The song dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

The log dataset consists of log files in JSON format generated by an event simulator based on the songs in the song dataset. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files are partitioned by year and month.

# Project files
The project includes four files:

- `create_table.py` creates the fact and dimension tables for the star schema in Redshift.
- `etl.py` loads data from S3 into staging tables on Redshift and then processes that data into analytics tables on Redshift.
- `sql_queries.py` defines the SQL statements, which will be imported into the files above.
- `README.md` is explains this process and the decisions for the ETL pipeline.

# Staging
The song data and log data are copied from S3 into Redshift staging tables.

The COPY command leverages the Amazon Redshift parallel processing architecture to read and load data in parallel by specifying a common prefix (`song_data` or `log_data`)

## Access Control
To load data from Amazon S3, COPY must have LIST access to the bucket and GET access for the bucket objects.

When you launch a redshift cluster create it with an IAM role that has read access to S3.
Add the redshift database name and the IAM role info to `dwh.cfg`.

## JSON Files

For the song data, the `auto` option is used and COPY automatically load fields from the JSON file.

For the log data, a JSONPaths file is needed since the column names and JSON field names don't match: the JSON field names contain uppercase characters and Redshift identifiers are case-insensitive and are folded to lowercase in the database.

The Udacity supplied log data file is `s3://udacity-dend/log_json_path.json`

## Schemas
The schema for the staging tables is shown below:

![ERD Diagram](https://github.com/troyjc/data-warehouse-project/blob/master/docs/Redshift%20Staging%20ERD.png)

# Running
I used a multi-node cluster with four nodes. After running `create_tables.py` to create the tables, I used the Query Editor to check the table schemas.

After running `etl.py` I looked at the fact and partition tables and ran some sample queries.

# Star Schema
There is one fact table named `songplay` and four dimension tables. The star schema is optimized for queries on song play analysis, so the `songplay` table uses a distribution style of key distribution for the fact table with the `user_id` column as the key. The `users` dimension table also uses key distribution with the `user_id` column as the key.

This approach of distributing the fact table and one dimension table on their common columns is suggested by Amazon.

The `artist` table is small, so it uses a distribution style of ALL.  

![ERD Diagram](https://github.com/troyjc/data-warehouse-project/blob/master/docs/Redshift%20Modeling%20ERD.png)

## Merge
Redshift doesn't have `upsert` functionality, so there were two problems to handle when building the dimension tables:
- Duplicate rows needed to be removed in order to maintain the unique constraint on the primary keys.
- The de-duped rows needed to be _merged_ into the dimension table (it didn't seem appropriate to assume that the dimension tables would be empty).

I used the suggested Amazon approach to perform a merge operation:
- Create a staging table and populate it with data to be merged.
- Use an inner join with the staging table to delete the rows from the target table that are being updated.
- Insert all of the rows from the staging table.
- Drop the staging table.

See `sql_queries.py` for the details.
